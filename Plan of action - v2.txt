# Comprehensive Plan of Action: Local Coding Assistant with Modern LLM + Smart Retrieval

## 1. **Project Objectives**

### Core Objectives
- **Primary Goal**: Deliver a conversational coding assistant that provides context-aware help for software development through natural language interaction
- **Key Capabilities**:
  - Syntax assistance with real-time error detection
  - Documentation lookup with semantic understanding
  - Code generation following latest best practices
  - Debugging guidance through conversational problem-solving
  - Example retrieval from indexed knowledge base
  - Multi-language support with specialized models

### Realistic Success Metrics
- Response latency < 1s for simple queries, < 3s for complex generation
- Memory footprint < 16GB under full load (with 7B model)
- 85%+ relevance for common coding queries
- Offline operation with optional online features
- Conversation context retained for 10+ turns (within model limits)

## 2. **System Architecture**

### High-Level Architecture
```
┌─────────────────────────────────────────────────────────────┐
│                        Web UI Layer                          │
│  [React + Monaco Editor + Markdown + Server-Sent Events]    │
└───────────────┬─────────────────────────┬───────────────────┘
                │                         │
┌───────────────▼───────────┐ ┌──────────▼────────────────────┐
│   API Gateway (FastAPI)   │ │   Static Asset Server         │
│   - SSE Handler           │ │   - UI Bundle (Vite)          │
│   - Request Router        │ │   - Syntax Themes             │
│   - Auth Middleware       │ │   - Wasm Components           │
└───────────────┬───────────┘ └───────────────────────────────┘
                │
┌───────────────▼────────────────────────────────────────────┐
│              Conversation Manager                           │
│  - Context Window Management (adaptive)                     │
│  - Message History with Vector Storage                      │
│  - Intent Classification (local classifier)                 │
│  - Response Streaming with Buffering                        │
└─────────┬───────────────────────────────┬──────────────────┘
          │                               │
┌─────────▼──────────┐      ┌────────────▼──────────────────┐
│  Inference Engine  │      │   Modern RAG Pipeline         │
│  - Model Registry  │      │   - HyDE Query Expansion      │
│  - Prompt Manager  │      │   - Multi-Stage Retrieval     │
│  - GPU Optimizer   │      │   - Self-RAG Validation       │
│  - Stream Handler  │      │   - Reranking Pipeline        │
└───────┬────────────┘      └─────────┬───────────────────┘
        │                              │
┌───────▼──────────────┐   ┌──────────▼────────────────────┐
│  Model Providers     │   │   Knowledge Base Layer        │
│  - llama.cpp         │   │   - Vector Store (LanceDB)    │
│  - ONNX Runtime      │   │   - FTS Index (Tantivy)       │
│  - Candle (Rust)     │   │   - Graph DB (for code deps)  │
│  - MLX (Apple)       │   │   - Metadata Store (DuckDB)   │
└──────────────────────┘   └─────────┬─────────────────────┘
                                      │
                          ┌───────────▼────────────────┐
                          │   Document Processing      │
                          │   - Tree-sitter Parser     │
                          │   - Markdown Processor     │
                          │   - Notebook Analyzer      │
                          │   - Smart Chunking         │
                          └────────────────────────────┘
```

### Component Updates

**API Gateway**
- FastAPI with native async/await
- Server-Sent Events for streaming 
- Request validation with Pydantic v2
- JWT auth with refresh tokens

**Conversation Manager**
- Adaptive context window based on model capabilities
- Sliding window with importance scoring
- Vector storage for long-term memory
- Real-time intent classification

**Modern Inference Engine**
- Multi-backend support (llama.cpp, ONNX, Candle)
- Dynamic GPU allocation
- Speculative decoding for 2x speedup
- Continuous batching for multiple users

**Enhanced RAG Pipeline**
- HyDE (Hypothetical Document Embeddings) for better retrieval
- Multi-stage retrieval with query expansion
- Self-RAG for answer validation
- Cross-encoder reranking with caching

## 3. **Technology Stack**

### Core Technologies

**UI Layer**
- **Framework**: React 18 with TypeScript 5
- **Code Editor**: Monaco Editor (VS Code's editor)
- **Styling**: Tailwind CSS + shadcn/ui components
- **State Management**: Valtio (proxy-based, simple)
- **Data Fetching**: TanStack Query
- **Markdown**: markdown-it with plugins
- **Build Tool**: Vite 5 (faster than webpack)

**Backend Services**
- **API Framework**: FastAPI with Pydantic v2
- **ASGI Server**: Uvicorn with uvloop
- **Background Tasks**: Python asyncio + Celery (optional)
- **Caching**: Redis-compatible (KeyDB for local)

**Search & Indexing**
- **Vector Database**: LanceDB (embedded, fast, modern)
- **Keyword Search**: Tantivy-py with BM25+
- **Embeddings**: BAAI/bge-small-en-v1.5 (general), microsoft/codebert-base (code)
- **Reranking**: cross-encoder/ms-marco-MiniLM-L-6-v2

**LLM Infrastructure**
```yaml
models:
  tier1_minimal:  # 8-12GB RAM
    name: Phi-3.5-mini-instruct
    size: 3.8B
    quantization: Q4_K_M
    context: 128k
    
  tier2_standard:  # 16-24GB RAM
    primary:
      name: Qwen2.5-Coder-7B-Instruct
      size: 7B
      quantization: Q4_K_M
      context: 32k
    alternative:
      name: DeepSeek-Coder-6.7B-Instruct
      size: 6.7B
      quantization: Q5_K_M
      context: 16k
      
  tier3_premium:  # 32GB+ RAM
    name: DeepSeek-Coder-33B-Instruct
    size: 33B
    quantization: Q4_K_M
    context: 16k
```

**Inference Backends**
- **Primary**: llama-cpp-python (broad model support)
- **Apple Silicon**: MLX (native Metal performance)
- **NVIDIA**: TensorRT-LLM (optional, maximum speed)
- **Fallback**: ONNX Runtime (cross-platform)

**Document Processing**
- **Code Parsing**: tree-sitter (all major languages)
- **Markdown**: mistune (fast, pure Python)
- **Notebooks**: nbformat + jupytext
- **PDFs**: pypdfium2 (modern, fast)

**Storage**
- **Database**: DuckDB (embedded OLAP, fast analytics)
- **Vector Persistence**: LanceDB (columnar storage)
- **Object Storage**: Local filesystem with zstd compression
- **Query Cache**: Disk-based with TTL

## 4. **User Interface Design**

### Modern Layout Structure
```
┌─────────────────────────────────────────────────────────────┐
│ Header Bar                                           [□ ○ ×] │
│ [≡] CodeAssist AI  [+New] [⌘K Search]         [◐] [Settings]│
├───────────┬─────────────────────────────────────────────────┤
│           │                                                 │
│ Sidebar   │            Main Chat Area                       │
│           │                                                 │
│ Chat      │  ┌───────────────────────────────────────┐    │
│ History   │  │ Welcome! How can I help you code?     │    │
│ ─────────│  │                                        │    │
│ Today     │  │ [User]: How do I implement a binary  │    │
│ • Debug.. │  │ search tree in Python?               │    │
│ • API...  │  │                                        │    │
│           │  │ [AI]: I'll help you implement a       │    │
│ Yesterday │  │ binary search tree. Here's a complete │    │
│ • Parse.. │  │ implementation:                       │    │
│           │  │                                        │    │
│ Bookmarks │  │ ```python                             │    │
│ ─────────│  │ class TreeNode:                       │    │
│ ★ Auth... │  │     def __init__(self, val=0):       │    │
│ ★ Data... │  │         self.val = val                │    │
│           │  │         self.left = None              │    │
│ Workspace │  │         self.right = None             │    │
│ ─────────│  │ ```                                   │    │
│ 📁 Project│  │ [Copy] [Run] [Explain]                │    │
│ 📄 Docs   │  └───────────────────────────────────────┘    │
│           │                                                 │
│           │  ┌───────────────────────────────────────┐    │
│           │  │ Type a message...           [@] [📎] ↵│    │
│           │  └───────────────────────────────────────┘    │
└───────────┴─────────────────────────────────────────────────┘
```

### Key UI Features
- **Real-time syntax highlighting** with language detection
- **Streaming responses** with smooth animation
- **Code actions toolbar**: Run, Copy, Insert, Explain, Improve
- **Context indicators**: Show active documents and model info
- **Smart suggestions**: Auto-complete for common queries
- **Multi-modal input**: Code paste, file upload, image analysis
- **Diff view**: For code improvements and fixes
- **Split view**: Chat + code editor side-by-side

### Accessibility & Performance
- **WCAG 2.1 AA** compliance
- **Keyboard navigation** throughout
- **Screen reader** friendly
- **60fps animations** with FLIP technique
- **Virtual scrolling** for long conversations
- **Progressive enhancement** (works without JS)

## 5. **Modern RAG Implementation**

### Enhanced Document Processing Pipeline

**Stage 1: Intelligent Parsing**
```python
class SmartDocumentParser:
    def __init__(self):
        self.parsers = {
            '.py': TreeSitterCodeParser(
                language='python',
                extract_docstrings=True,
                extract_types=True,
                extract_imports=True
            ),
            '.md': MarkdownParser(
                extract_code_blocks=True,
                preserve_structure=True
            ),
            '.ipynb': NotebookParser(
                include_markdown=True,
                include_outputs='text_only'
            )
        }
    
    async def parse(self, file_path: Path) -> Document:
        parser = self.parsers[file_path.suffix]
        content = await parser.parse(file_path)
        
        # Extract semantic structure
        structure = await self.extract_structure(content)
        
        # Generate metadata
        metadata = {
            'language': parser.language,
            'imports': content.imports,
            'classes': content.classes,
            'functions': content.functions,
            'complexity': calculate_complexity(content)
        }
        
        return Document(content, structure, metadata)
```

**Stage 2: Semantic Chunking**
```python
class SemanticChunker:
    def __init__(self, model_name='BAAI/bge-small-en-v1.5'):
        self.encoder = SentenceTransformer(model_name)
        self.max_chunk_size = 512
        self.overlap_size = 64
        
    async def chunk(self, document: Document) -> List[Chunk]:
        # Use semantic similarity for split points
        sentences = self.sentence_splitter.split(document.content)
        embeddings = self.encoder.encode(sentences)
        
        # Find optimal split points using embedding similarity
        split_points = self.find_semantic_boundaries(
            embeddings, 
            min_size=256,
            max_size=self.max_chunk_size
        )
        
        chunks = []
        for start, end in split_points:
            chunk = Chunk(
                content=sentences[start:end],
                embedding=embeddings[start:end].mean(axis=0),
                metadata={
                    'doc_id': document.id,
                    'position': f"{start}-{end}",
                    'context': self.extract_context(document, start, end)
                }
            )
            chunks.append(chunk)
            
        return chunks
```

**Stage 3: Multi-Index Strategy**
```python
class MultiIndexer:
    def __init__(self):
        self.vector_db = lancedb.connect("./vector_store")
        self.fts_engine = TantivyIndex("./fts_index")
        self.graph_db = CodeGraphDB("./graph_store")
        
    async def index_document(self, doc: Document, chunks: List[Chunk]):
        # Vector index for semantic search
        await self.index_vectors(chunks)
        
        # Full-text search for exact matches
        await self.index_keywords(doc, chunks)
        
        # Code graph for relationship queries
        if doc.is_code:
            await self.index_code_graph(doc)
    
    async def index_vectors(self, chunks: List[Chunk]):
        # Create embeddings with different models
        embeddings = {
            'general': self.general_encoder.encode(chunks),
            'code': self.code_encoder.encode(chunks) if is_code else None
        }
        
        # Store in LanceDB with metadata
        self.vector_db.create_table(
            "documents",
            data=[{
                "vector_general": e['general'],
                "vector_code": e['code'],
                "text": c.content,
                "metadata": c.metadata
            } for c, e in zip(chunks, embeddings)]
        )
```

### Advanced Retrieval Pipeline

**HyDE Implementation**
```python
class HyDERetriever:
    def __init__(self, generator_model):
        self.generator = generator_model
        self.template = """Generate a detailed answer to this question:
        Question: {question}
        
        Provide code examples and explanations as if from documentation:"""
    
    async def retrieve(self, query: str, k: int = 10) -> List[Document]:
        # Generate hypothetical document
        prompt = self.template.format(question=query)
        hypothetical_doc = await self.generator.generate(prompt, max_tokens=512)
        
        # Embed the hypothetical document
        hyde_embedding = self.encoder.encode(hypothetical_doc)
        
        # Search using the enhanced embedding
        results = await self.vector_db.search(
            hyde_embedding,
            k=k * 2,
            include_original=True
        )
        
        return self.rerank_results(query, results, k)
```

**Self-RAG Validation**
```python
class SelfRAGPipeline:
    async def retrieve_and_validate(self, query: str) -> ValidatedResponse:
        # Initial retrieval
        candidates = await self.retriever.retrieve(query, k=20)
        
        # Generate answer with citations
        answer = await self.generator.generate_with_citations(
            query, candidates
        )
        
        # Self-assessment
        relevance_check = await self.assess_relevance(query, answer)
        
        if relevance_check.score < 0.8:
            # Refine search and regenerate
            refined_query = await self.refine_query(query, answer)
            candidates = await self.retriever.retrieve(refined_query, k=20)
            answer = await self.generator.generate_with_citations(
                query, candidates
            )
        
        # Fact verification
        facts = self.extract_claims(answer)
        verified_facts = await self.verify_facts(facts, candidates)
        
        return ValidatedResponse(
            answer=answer,
            confidence=self.calculate_confidence(verified_facts),
            sources=candidates[:5]
        )
```

## 6. **Optimized Storage & State Management**

### Modern Database Schema
```sql
-- Using DuckDB for analytics and metadata
CREATE TABLE conversations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id TEXT NOT NULL,
    title TEXT,
    model_used TEXT,
    total_tokens INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT now(),
    _at TIMESTAMP DEFAULT now(),
    vector_embedding FLOAT[384],  -- For similarity search
    metadata JSON
);

CREATE TABLE messages (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    conversation_id UUID REFERENCES conversations(id),
    role TEXT CHECK(role IN ('user', 'assistant', 'system')),
    content TEXT,
    tokens INTEGER,
    timestamp TIMESTAMP DEFAULT now(),
    feedback INTEGER CHECK(feedback IN (-1, 0, 1)),
    metadata JSON,
    embedding FLOAT[384]
);

-- Efficient search across conversations
CREATE INDEX idx_message_embedding ON messages 
USING HNSW(embedding) WITH (metric = 'cosine');

-- Fast text search
CREATE INDEX idx_message_content ON messages 
USING FTS(content);

-- Code snippets for reuse
CREATE TABLE code_snippets (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    message_id UUID REFERENCES messages(id),
    language TEXT,
    code TEXT,
    explanation TEXT,
    tags TEXT[],
    usage_count INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT now()
);
```

### State Management Architecture
```python
class ConversationStateManager:
    def __init__(self, db_path: str):
        self.db = duckdb.connect(db_path)
        self.cache = TTLCache(maxsize=1000, ttl=3600)
        self.vector_store = lancedb.connect("./conversations")
        
    async def save_message(self, conversation_id: str, message: Message):
        # Compute embedding for semantic search
        embedding = await self.encoder.encode(message.content)
        
        # Save to DuckDB
        self.db.execute("""
            INSERT INTO messages 
            (conversation_id, role, content, tokens, embedding)
            VALUES (?, ?, ?, ?, ?)
        """, [conversation_id, message.role, message.content, 
              message.tokens, embedding])
        
        # Update conversation summary
        await self.update_conversation_summary(conversation_id)
        
        # Update vector index for similarity search
        await self.vector_store.add({
            'id': message.id,
            'vector': embedding,
            'conversation_id': conversation_id,
            'content': message.content
        })
    
    async def search_conversations(
        self, 
        query: str, 
        user_id: str,
        limit: int = 10
    ) -> List[Conversation]:
        # Hybrid search: vector + keyword
        query_embedding = await self.encoder.encode(query)
        
        # Vector search
        vector_results = await self.vector_store.search(
            query_embedding,
            filter=f"user_id = '{user_id}'",
            limit=limit * 2
        )
        
        # Keyword search
        keyword_results = self.db.execute("""
            SELECT DISTINCT conversation_id, 
                   ts_rank(content, plainto_tsquery(?)) as rank
            FROM messages
            WHERE user_id = ? 
              AND content @@ plainto_tsquery(?)
            ORDER BY rank DESC
            LIMIT ?
        """, [query, user_id, query, limit * 2]).fetchall()
        
        # Combine and rerank
        return self.combine_search_results(
            vector_results, 
            keyword_results, 
            limit
        )
```

## 7. **Performance Optimization**

### GPU Optimization Strategy
```python
class GPUOptimizer:
    def __init__(self):
        self.device = self.detect_best_device()
        
    def detect_best_device(self):
        # Apple Silicon
        if platform.system() == "Darwin" and platform.processor() == "arm":
            return AppleSiliconBackend()
        
        # NVIDIA
        if torch.cuda.is_available():
            return CUDABackend(
                use_flash_attention=True,
                use_triton=True
            )
        
        # AMD
        if self.check_rocm():
            return ROCmBackend()
        
        # Intel
        if self.check_intel_gpu():
            return IntelXPUBackend()
        
        # Fallback to CPU with optimizations
        return CPUBackend(
            use_onnx=True,
            num_threads=os.cpu_count()
        )

class AppleSiliconBackend:
    def __init__(self):
        import mlx
        self.mlx = mlx
        
    def load_model(self, model_path: str):
        # Use MLX for native Metal performance
        model = self.mlx.load_model(model_path)
        model = self.mlx.compile(model)  # JIT compilation
        return model
```

### Inference Optimization
```python
class OptimizedInferenceEngine:
    def __init__(self, model_config):
        self.config = model_config
        self.setup_optimizations()
        
    def setup_optimizations(self):
        # Enable Flash Attention 2 if available
        if self.config.get('use_flash_attention'):
            from flash_attn import flash_attn_func
            self.attention_fn = flash_attn_func
        
        # Setup speculative decoding for 2x speedup
        if self.config.get('use_speculative'):
            self.draft_model = self.load_draft_model()
        
        # Continuous batching for multiple users
        self.batch_manager = ContinuousBatchingManager(
            max_batch_size=8,
            max_sequence_length=self.config['context_length']
        )
    
    async def generate_stream(
        self, 
        prompt: str, 
        **kwargs
    ) -> AsyncIterator[str]:
        # Add to batch
        request_id = self.batch_manager.add_request(prompt)
        
        # Use vLLM-style continuous batching
        async for token in self.batch_manager.process_request(request_id):
            yield token
            
            # Speculative decoding
            if hasattr(self, 'draft_model'):
                # Generate multiple tokens at once
                draft_tokens = self.draft_model.generate(
                    context=self.batch_manager.get_context(request_id),
                    n_tokens=4
                )
                
                # Verify with main model
                verified = await self.verify_tokens(draft_tokens)
                if verified:
                    yield from verified
```

### Memory-Efficient Loading
```python
class MemoryEfficientModelLoader:
    def __init__(self, memory_limit_gb: int = 16):
        self.memory_limit = memory_limit_gb * 1024 * 1024 * 1024
        self.loaded_models = OrderedDict()
        
    async def load_model(self, model_name: str, **kwargs):
        model_info = self.get_model_info(model_name)
        
        # Check if we need to free memory
        while self.get_current_usage() + model_info['size'] > self.memory_limit:
            # Unload least recently used
            lru_model = next(iter(self.loaded_models))
            await self.unload_model(lru_model)
        
        # Load with optimal settings
        if model_info['size'] < 4_000_000_000:  # < 4GB
            # Load entire model
            model = self.load_full_model(model_name)
        else:
            # Use memory mapping and lazy loading
            model = self.load_mmap_model(model_name)
        
        self.loaded_models[model_name] = model
        return model
    
    def load_mmap_model(self, model_name: str):
        """Load model with memory mapping for large models"""
        return llama_cpp.Llama(
            model_path=f"models/{model_name}.gguf",
            n_ctx=8192,
            n_batch=512,
            n_threads=os.cpu_count(),
            use_mmap=True,
            use_mlock=False,  # Don't lock in RAM
            low_vram=True     # Minimize VRAM usage
        )
```

## 8. **Security & Privacy Enhancements**

### Network Isolation Policy
```python
class NetworkSecurityPolicy:
    def __init__(self, config):
        self.allowed_domains = config.get('allowed_domains', [])
        self.block_all = config.get('block_all_network', True)
        
    def create_secure_session(self):
        """Create session with network restrictions"""
        import urllib3
        
        class SecureHTTPAdapter(urllib3.HTTPAdapter):
            def send(self, request, **kwargs):
                url = urlparse(request.url)
                
                # Check if domain is allowed
                if self.policy.block_all:
                    raise SecurityError("Network access is disabled")
                
                if url.netloc not in self.policy.allowed_domains:
                    raise SecurityError(f"Access to {url.netloc} blocked")
                
                return super().send(request, **kwargs)
        
        session = requests.Session()
        session.mount('http://', SecureHTTPAdapter())
        session.mount('https://', SecureHTTPAdapter())
        return session
```

### Data Encryption Layer
```python
class EncryptionManager:
    def __init__(self):
        self.key = self.derive_key_from_password()
        
    def derive_key_from_password(self, password: str = None):
        """Derive encryption key from user password"""
        if not password:
            password = getpass.getpass("Enter encryption password: ")
        
        salt = b'local-assistant-salt'  # Should be random in production
        kdf = PBKDF2HMAC(
            algorithm=hashes.SHA256(),
            length=32,
            salt=salt,
            iterations=100000,
        )
        return base64.urlsafe_b64encode(kdf.derive(password.encode()))
    
    def encrypt_conversation(self, conversation: dict) -> bytes:
        """Encrypt conversation data"""
        f = Fernet(self.key)
        return f.encrypt(json.dumps(conversation).encode())
    
    def setup_encrypted_db(self):
        """Setup SQLCipher encrypted database"""
        conn = sqlite3.connect('file:assistant.db?cipher=sqlcipher')
        conn.execute(f"PRAGMA key = '{self.key.decode()}'")
        conn.execute("PRAGMA cipher_page_size = 4096")
        conn.execute("PRAGMA kdf_iter = 100000")
        return conn
```

## 9. **Testing & Quality Assurance**

### Comprehensive Test Suite
```python
# tests/test_inference.py
class TestInferencePerformance:
    @pytest.mark.benchmark
    def test_simple_query_latency(self, benchmark):
        """Test simple query stays under 1s"""
        engine = create_test_engine()
        
        result = benchmark(
            engine.generate,
            "What is a Python list?",
            max_tokens=100
        )
        
        assert benchmark.stats['mean'] < 1.0  # Under 1 second
        assert len(result) > 50  # Meaningful response
    
    @pytest.mark.benchmark
    def test_streaming_latency(self, benchmark):
        """Test time to first token in streaming"""
        engine = create_test_engine()
        tokens = []
        
        def get_first_token():
            for token in engine.generate_stream("Write a function"):
                tokens.append(token)
                return token  # Return immediately after first
        
        first_token = benchmark(get_first_token)
        assert benchmark.stats['mean'] < 0.2  # Under 200ms

# tests/test_retrieval.py
class TestRAGAccuracy:
    def test_code_retrieval_accuracy(self):
        """Test retrieval accuracy for code queries"""
        test_cases = load_test_dataset('code_queries.json')
        retriever = create_test_retriever()
        
        total_score = 0
        for case in test_cases:
            results = retriever.search(case['query'], k=5)
            
            # Check if correct document is in top 5
            found = any(
                r.doc_id in case['expected_docs'] 
                for r in results
            )
            
            if found:
                # Calculate position-based score
                position = next(
                    i for i, r in enumerate(results)
                    if r.doc_id in case['expected_docs']
                )
                score = 1.0 / (position + 1)  # 1 for first, 0.5 for second, etc.
                total_score += score
        
        accuracy = total_score / len(test_cases)
        assert accuracy > 0.85  # 85% accuracy target
```

### Integration Testing
```python
# tests/test_e2e.py
class TestEndToEnd:
    @pytest.mark.asyncio
    async def test_conversation_flow(self):
        """Test complete conversation flow"""
        client = TestClient(app)
        
        # Start conversation
        response = await client.post("/conversations/create")
        conv_id = response.json()['id']
        
        # Send message
        sse_client = SSEClient(
            f"/conversations/{conv_id}/messages",
            json={"content": "How do I read a CSV in pandas?"}
        )
        
        chunks = []
        async for chunk in sse_client:
            chunks.append(chunk.data)
        
        full_response = ''.join(chunks)
        assert 'pd.read_csv' in full_response
        assert len(full_response) > 100
        
    @pytest.mark.asyncio  
    async def test_document_upload_and_search(self):
        """Test document processing pipeline"""
        client = TestClient(app)
        
        # Upload document
        with open('test_doc.py', 'rb') as f:
            response = await client.post(
                "/documents/upload",
                files={"file": f}
            )
        
        doc_id = response.json()['id']
        
        # Wait for processing
        await asyncio.sleep(2)
        
        # Search for content
        search_response = await client.get(
            "/search",
            params={"q": "test function", "doc_id": doc_id}
        )
        
        results = search_response.json()
        assert len(results) > 0
        assert results[0]['score'] > 0.7
```

### Performance Monitoring
```python
class PerformanceMonitor:
    def __init__(self):
        self.metrics = {
            'response_times': deque(maxlen=1000),
            'memory_usage': deque(maxlen=1000),
            'cache_hits': 0,
            'cache_misses': 0
        }
    
    async def measure_request(self, func, *args, **kwargs):
        start_time = time.perf_counter()
        start_memory = psutil.Process().memory_info().rss
        
        try:
            result = await func(*args, **kwargs)
            
            # Record metrics
            elapsed = time.perf_counter() - start_time
            memory_delta = psutil.Process().memory_info().rss - start_memory
            
            self.metrics['response_times'].append(elapsed)
            self.metrics['memory_usage'].append(memory_delta)
            
            # Alert if performance degrades
            if elapsed > 3.0:  # 3 second threshold
                logger.warning(f"Slow request: {elapsed:.2f}s for {func.__name__}")
            
            return result
            
        except Exception as e:
            logger.error(f"Request failed: {e}")
            raise
    
    def get_percentiles(self):
        times = list(self.metrics['response_times'])
        return {
            'p50': np.percentile(times, 50),
            'p95': np.percentile(times, 95),
            'p99': np.percentile(times, 99)
        }
```

## 10. **Deployment & Distribution**

### Multi-Platform Packaging

**Desktop Application (Tauri - Rust-based, lighter than Electron)**
```toml
# tauri.conf.json
{
  "package": {
    "productName": "CodeAssist AI",
    "version": "1.0.0"
  },
  "tauri": {
    "bundle": {
      "active": true,
      "targets": ["app", "dmg", "msi", "appimage", "deb"],
      "identifier": "ai.codeassist.local",
      "icon": ["icons/icon.png"],
      "resources": ["models/*", "data/*"],
      "copyright": "© 2025 CodeAssist AI",
      "category": "DeveloperTool"
    },
    "security": {
      "csp": "default-src 'self'; script-src 'self'"
    }
  }
}
```

**Progressive Web App**
```javascript
// manifest.json
{
  "name": "CodeAssist AI",
  "short_name": "CodeAssist",
  "description": "Local AI coding assistant",
  "start_url": "/",
  "display": "standalone",
  "background_color": "#1a1a1a",
  "theme_color": "#0ea5e9",
  "icons": [
    {
      "src": "/icon-192.png",
      "sizes": "192x192",
      "type": "image/png"
    }
  ],
  "protocol_handlers": [
    {
      "protocol": "codeassist",
      "url": "/handle/%s"
    }
  ]
}
```

**Docker Deployment**
```dockerfile
# Multi-stage build for efficiency
FROM python:3.11-slim as builder

# Install build dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    cmake \
    git \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .
RUN pip wheel --no-cache-dir --no-deps --wheel-dir /wheels -r requirements.txt

# Runtime stage
FROM python:3.11-slim

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    libgomp1 \
    && rm -rf /var/lib/apt/lists/*

# Copy wheels and install
COPY --from=builder /wheels /wheels
RUN pip install --no-cache /wheels/*

# Copy application
COPY . /app
WORKDIR /app

# Create volume for models
VOLUME ["/app/models", "/app/data"]

# Health check
HEALTHCHECK --interval=30s --timeout=3s \
  CMD curl -f http://localhost:8080/health || exit 1

EXPOSE 8080

CMD ["python", "-m", "uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8080"]
```

### Model Distribution Strategy
```python
class ModelDistributor:
    """Handle model downloads and updates"""
    
    def __init__(self):
        self.sources = {
            'huggingface': HuggingFaceSource(),
            'local_mirror': LocalMirrorSource(),
            'torrent': TorrentSource()  # For large models
        }
    
    async def download_model(
        self, 
        model_name: str,
        source: str = 'auto'
    ) -> Path:
        # Auto-select best source
        if source == 'auto':
            source = await self.select_best_source(model_name)
        
        # Download with progress
        progress = tqdm(desc=f"Downloading {model_name}")
        
        async for chunk in self.sources[source].download(model_name):
            progress.update(len(chunk))
            yield chunk
        
        # Verify integrity
        if not await self.verify_model(model_name):
            raise ValueError("Model verification failed")
        
        return self.get_model_path(model_name)
```

### First-Run Setup Wizard
```python
class SetupWizard:
    def __init__(self):
        self.steps = [
            SystemCheckStep(),
            ModelSelectionStep(),
            LanguageSelectionStep(),
            PerformanceTestStep(),
            CompletionStep()
        ]
    
    async def run(self):
        print("Welcome to CodeAssist AI Setup!\n")
        
        for i, step in enumerate(self.steps):
            print(f"Step {i+1}/{len(self.steps)}: {step.name}")
            print("-" * 50)
            
            result = await step.execute()
            
            if not result.success:
                if await self.prompt_retry():
                    i -= 1  # Retry this step
                else:
                    return False
        
        print("\n✅ Setup complete! Starting CodeAssist AI...")
        return True

class ModelSelectionStep:
    name = "Select AI Model"
    
    async def execute(self):
        system_info = get_system_info()
        recommendations = self.get_recommendations(system_info)
        
        print(f"System detected: {system_info['ram_gb']}GB RAM, "
              f"{system_info['gpu'] or 'No GPU'}")
        print("\nRecommended models:")
        
        for i, model in enumerate(recommendations):
            print(f"{i+1}. {model['name']} ({model['size']})")
            print(f"   - {model['description']}")
            print(f"   - Requirements: {model['requirements']}")
        
        choice = await prompt_choice("Select model (1-3): ", len(recommendations))
        selected = recommendations[choice - 1]
        
        # Download selected model
        await self.download_model(selected)
        
        return StepResult(success=True, data={'model': selected})
```

##  Implementation Timeline

### Month 1: Foundation & MVP
- **Week 1**: Project setup, core API structure
- **Week 2**: Basic LLM integration (Phi-3.5 for quick start)
- **Week 3**: Simple chat UI with streaming
- **Week 4**: Basic conversation management

### Month 2: RAG Implementation
- **Week 1**: Document parsing pipeline
- **Week 2**: Vector store setup (LanceDB)
- **Week 3**: Hybrid search implementation
- **Week 4**: RAG integration with chat

### Month 3: Performance & Polish
- **Week 1**: GPU optimization for different platforms
- **Week 2**: Implement streaming improvements
- **Week 3**: Memory optimization and caching
- **Week 4**: UI polish and error handling

### Month 4: Advanced Features
- **Week 1**: Multi-language support
- **Week 2**: Code execution sandboxing
- **Week 3**: Advanced search features
- **Week 4**: Testing and benchmarking

### Month 5: Production Ready
- **Week 1**: Security hardening
- **Week 2**: Performance testing at scale
- **Week 3**: Documentation and examples
- **Week 4**: Package for distribution

### Month 6: Community & Extensions
- **Week 1**: Plugin system implementation
- **Week 2**: Model marketplace
- **Week 3**: Community features
- **Week 4**: Launch preparation

## Success Metrics (Revised)

### Performance Targets
- **Response Time**: 
  - Simple queries: < 1s (p95)
  - Code generation: < 3s (p95)
  - First token: < 200ms (p95)
- **Memory Usage**: < 16GB with 7B model loaded
- **Accuracy**: 85%+ relevance for common queries

### User Experience
- **Setup Time**: < 10 minutes (excluding model download)
- **Model Download**: Resumable, torrent option for large models
- **Crash Rate**: < 0.1%
- **User Satisfaction**: > 4.5/5 rating

### Technical Metrics
- **Test Coverage**: > 80%
- **Documentation**: 100% API coverage
- **Platform Support**: Windows, macOS, Linux
- **Language Support**: Python, JavaScript, TypeScript, Go (at launch)

This  plan incorporates modern technologies, realistic performance targets, and a practical implementation approach while maintaining the original vision of a powerful, privacy-focused local coding assistant.